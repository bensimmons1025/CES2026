{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import csv\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pypfopt import EfficientFrontier\n",
    "from pypfopt import risk_models\n",
    "from pypfopt import expected_returns\n",
    "from pypfopt import objective_functions\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (MultiHeadAttention, EncoderLayer 클래스는 기존 코드와 동일하게 사용)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.depth = d_model // num_heads\n",
    "\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        self.W_O = nn.Linear(d_model, d_model)\n",
    " \n",
    "    def forward(self, Q, K, V):\n",
    "        Q = self.W_Q(Q)\n",
    "        K = self.W_K(K)\n",
    "        V = self.W_V(V)\n",
    "\n",
    "        Q = self._split_heads(Q)\n",
    "        K = self._split_heads(K)\n",
    "        V = self._split_heads(V)\n",
    "\n",
    "        attention_weights = torch.matmul(Q, K.transpose(-1, -2)) / torch.sqrt(torch.tensor(self.depth, dtype=torch.float32))\n",
    "        attention_weights = torch.softmax(attention_weights, dim=-1)\n",
    "\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        output = self._combine_heads(output)\n",
    "\n",
    "        output = self.W_O(output)\n",
    "        return output\n",
    " \n",
    "    def _split_heads(self, tensor):\n",
    "        tensor = tensor.view(tensor.size(0), -1, self.num_heads, self.depth)\n",
    "        return tensor.transpose(1, 2)\n",
    " \n",
    "    def _combine_heads(self, tensor):\n",
    "        tensor = tensor.transpose(1, 2).contiguous()\n",
    "        return tensor.view(tensor.size(0), -1, self.num_heads * self.depth)\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * d_model, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    " \n",
    "    def forward(self, x):\n",
    "        attention_output = self.attention(x, x, x)\n",
    "        attention_output = self.norm1(x + attention_output)\n",
    "\n",
    "        feedforward_output = self.feedforward(attention_output)\n",
    "        output = self.norm2(attention_output + feedforward_output)\n",
    "        return output\n",
    "\n",
    "# [!!! 모델 수정 !!!]\n",
    "class iTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    iTransformer (Encoder-Only)\n",
    "    - input_len (seq_len): 입력 시퀀스 길이 (예: 1024)\n",
    "    - output_len (pred_len): 예측 시퀀스 길이 (예: 14)\n",
    "    - num_features (n_vars): 변수 개수 (예: 5)\n",
    "    - hidden_dim (d_model): 모델의 임베딩 차원 (예: 64)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_len, output_len, num_features, hidden_dim, num_heads, num_layers):\n",
    "        super(iTransformer, self).__init__()\n",
    "        \n",
    "        # [수정] 입력 레이어: (Batch, N, L) -> (Batch, N, d_model)\n",
    "        # L = input_len, N = num_features\n",
    "        self.input_layer = nn.Linear(input_len, hidden_dim)\n",
    "        \n",
    "        # [추가] 변수(토큰)를 위한 Positional Embedding\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, num_features, hidden_dim))\n",
    "        \n",
    "        # 인코더 레이어 (기존과 동일)\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(hidden_dim, num_heads) for _ in range(num_layers)])\n",
    "        \n",
    "        # [수정] 출력 레이어: (Batch, N, d_model) -> (Batch, N, P)\n",
    "        # P = output_len\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_len)\n",
    " \n",
    "    def forward(self, x):\n",
    "        # x의 입력 형태: (Batch, L, N) = (Batch, 1024, 5)\n",
    "        \n",
    "        # 1. 전치 (Transpose): (B, L, N) -> (B, N, L)\n",
    "        # 변수(N)를 토큰(시퀀스) 차원으로, 시간(L)을 임베딩 차원으로\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # 2. 임베딩: (B, N, L) -> (B, N, d_model)\n",
    "        x = self.input_layer(x)\n",
    "        \n",
    "        # 3. Positional Embedding 추가\n",
    "        x = x + self.pos_embedding\n",
    "\n",
    "        # 4. 인코더 (어텐션은 N x N (5x5) 차원에서 수행됨)\n",
    "        encoder_output = x\n",
    "        for layer in self.encoder_layers:\n",
    "            encoder_output = layer(encoder_output)\n",
    "\n",
    "        # 5. 출력 레이어 (프로젝션): (B, N, d_model) -> (B, N, P)\n",
    "        decoder_output = self.output_layer(encoder_output)\n",
    "        \n",
    "        # 6. 최종 전치 (Transpose): (B, N, P) -> (B, P, N)\n",
    "        # (Batch, Pred_Len, Num_Features) 형태로 변환\n",
    "        output = decoder_output.permute(0, 2, 1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "--- 모든 주식 데이터 통합 ---\n",
      "통합 데이터 형태: (1256, 50)\n",
      "--- 단일 통합 스케일러 훈련 ---\n",
      "'unified_scaler.pkl' 저장 완료.\n",
      "'unified_column_map.pkl' (컬럼 순서) 저장 완료.\n",
      "--- 슬라이딩 윈도우 생성 (50 features) ---\n",
      "Inputs shape: torch.Size([219, 1024, 50]), Outputs shape: torch.Size([219, 14, 50])\n",
      "--- 단일 통합 모델 훈련 ---\n",
      "Epoch [10/100], Loss: 0.136936\n",
      "Epoch [20/100], Loss: 0.095452\n",
      "Epoch [30/100], Loss: 0.071136\n",
      "Epoch [40/100], Loss: 0.054976\n",
      "Epoch [50/100], Loss: 0.042844\n",
      "Epoch [60/100], Loss: 0.034745\n",
      "Epoch [70/100], Loss: 0.027922\n",
      "Epoch [80/100], Loss: 0.022842\n",
      "Epoch [90/100], Loss: 0.019123\n",
      "Epoch [100/100], Loss: 0.015672\n",
      "'unified_model.pt' 저장 완료.\n"
     ]
    }
   ],
   "source": [
    "csv_files = [\n",
    "    'BROADCOM 5년치.csv', 'ALPHABET C 5년치.csv', 'AMAZON 5년치.csv', \n",
    "    'APPLE 5년치.csv', 'META 5년치.csv', 'MICROSOFT 5년치.csv', \n",
    "    'NETFLIX 5년치.csv', 'NVIDIA 5년치.csv', 'PALANTIR 5년치.csv', 'TESLA 5년치.csv'\n",
    "]\n",
    "feature_cols = ['Close/Last', 'Volume', 'Open', 'High', 'Low']\n",
    "\n",
    "# [수정] num_features = 10개 주식 * 5개 변수 = 50개\n",
    "num_features = len(csv_files) * len(feature_cols) \n",
    "\n",
    "input_len = 1024\n",
    "output_len = 14\n",
    "hidden_dim = 128 # (50개 변수를 다루므로 128로 늘려도 좋습니다)\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "num_epochs = 100\n",
    "batch_size = 16\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 1. [수정] 10개 주식 데이터를 하나의 (Total_Len, 50) DataFrame으로 통합 ---\n",
    "print(\"--- 모든 주식 데이터 통합 ---\")\n",
    "all_data_dfs = []\n",
    "column_names = [] # 50개 컬럼 이름 저장\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    stock_name = re.match(r'^[A-Z]+', csv_file).group(0)\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(csv_file, parse_dates=['Date'], index_col='Date')\n",
    "        df = df.sort_index(ascending=True)\n",
    "        \n",
    "        # 클리닝\n",
    "        cleaned_cols = {}\n",
    "        for col in feature_cols:\n",
    "            series = df[col]\n",
    "            if series.dtype == 'object':\n",
    "                series = series.str.replace(r'[$,]', '', regex=True)\n",
    "            series = pd.to_numeric(series, errors='coerce')\n",
    "            \n",
    "            # 컬럼 이름 변경 (예: APPLE_Close/Last, APPLE_Volume)\n",
    "            new_col_name = f\"{stock_name}_{col}\"\n",
    "            cleaned_cols[new_col_name] = series\n",
    "            column_names.append(new_col_name)\n",
    "            \n",
    "        all_data_dfs.append(pd.DataFrame(cleaned_cols))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"오류: {csv_file} 처리 중 - {e}\")\n",
    "\n",
    "# Date 인덱스 기준으로 모든 DataFrame을 옆으로(axis=1) 합침\n",
    "all_data_wide_df = pd.concat(all_data_dfs, axis=1)\n",
    "\n",
    "# [중요] 모든 주식이 상장되어 데이터가 있는 날짜만 사용 (NaN 행 제거)\n",
    "all_data_wide_df = all_data_wide_df.dropna()\n",
    "\n",
    "all_data_np = all_data_wide_df.to_numpy(dtype=np.float32)\n",
    "print(f\"통합 데이터 형태: {all_data_np.shape}\") # (N_days, 50)\n",
    "\n",
    "if len(all_data_np) < input_len + output_len:\n",
    "    raise ValueError(\"데이터 부족: 모든 주식이 동시에 상장된 기간이 너무 짧습니다.\")\n",
    "\n",
    "# --- 2. [수정] 단일 스케일러 훈련 및 저장 ---\n",
    "print(\"--- 단일 통합 스케일러 훈련 ---\")\n",
    "scaler = StandardScaler()\n",
    "scaled_data_np = scaler.fit_transform(all_data_np) # (N_days, 50)\n",
    "\n",
    "scaler_path = \"unified_scaler.pkl\"\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"'{scaler_path}' 저장 완료.\")\n",
    "\n",
    "# (참고) 50개 컬럼의 순서 저장\n",
    "column_map_path = \"unified_column_map.pkl\"\n",
    "with open(column_map_path, 'wb') as f:\n",
    "    pickle.dump(column_names, f)\n",
    "print(f\"'{column_map_path}' (컬럼 순서) 저장 완료.\")\n",
    "\n",
    "# --- 3. [수정] 단일 윈도우 생성 (50개 변수) ---\n",
    "print(\"--- 슬라이딩 윈도우 생성 (50 features) ---\")\n",
    "inputs, outputs = [], []\n",
    "total_len = len(scaled_data_np)\n",
    "for i in range(total_len - input_len - output_len + 1):\n",
    "    inputs.append(scaled_data_np[i : i + input_len, :])\n",
    "    outputs.append(scaled_data_np[i + input_len : i + input_len + output_len, :])\n",
    "\n",
    "inputs = torch.tensor(np.array(inputs), dtype=torch.float32)\n",
    "outputs = torch.tensor(np.array(outputs), dtype=torch.float32)\n",
    "\n",
    "dataset = TensorDataset(inputs, outputs)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "print(f\"Inputs shape: {inputs.shape}, Outputs shape: {outputs.shape}\")\n",
    "\n",
    "# --- 4. [수정] 단일 통합 모델 훈련 ---\n",
    "print(\"--- 단일 통합 모델 훈련 ---\")\n",
    "model = iTransformer(\n",
    "    input_len=input_len, \n",
    "    output_len=output_len, \n",
    "    num_features=num_features, # 50\n",
    "    hidden_dim=hidden_dim, \n",
    "    num_heads=num_heads, \n",
    "    num_layers=num_layers\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs_batch, labels_batch in data_loader:\n",
    "        inputs_batch, labels_batch = inputs_batch.to(device), labels_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs_batch = model(inputs_batch) # (Batch, 14, 50)\n",
    "        loss = criterion(outputs_batch, labels_batch)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(data_loader):.6f}')\n",
    "\n",
    "# --- 5. [수정] 단일 모델 저장 ---\n",
    "model_path = \"unified_model.pt\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"'{model_path}' 저장 완료.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 모델(hidden_dim=128) 및 스케일러 로드 ---\n",
      "--- 추론용 데이터 준비 ---\n",
      "--- 예측 수행 ---\n",
      "--- 1. 기대수익률 (mu) 계산 ---\n",
      "  [BROADCOM] 현재가: 369.63, 14일 후 예측: 353.10 -> 연 -56.10%\n",
      "  [ALPHABET] 현재가: 281.82, 14일 후 예측: 256.73 -> 연 -81.33%\n",
      "  [AMAZON] 현재가: 244.22, 14일 후 예측: 220.37 -> 연 -84.27%\n",
      "  [APPLE] 현재가: 270.37, 14일 후 예측: 268.26 -> 연 -13.18%\n",
      "  [META] 현재가: 648.35, 14일 후 예측: 715.90 -> 연 495.28%\n",
      "  [MICROSOFT] 현재가: 517.81, 14일 후 예측: 509.74 -> 연 -24.64%\n",
      "  [NETFLIX] 현재가: 1118.86, 14일 후 예측: 1156.65 -> 연 81.82%\n",
      "  [NVIDIA] 현재가: 202.49, 14일 후 예측: 197.06 -> 연 -38.67%\n",
      "  [PALANTIR] 현재가: 200.47, 14일 후 예측: 180.05 -> 연 -85.53%\n",
      "  [TESLA] 현재가: 456.56, 14일 후 예측: 449.79 -> 연 -23.57%\n",
      "--- 2. 위험 공분산 (S) 계산 (과거 종가 데이터 사용) ---\n",
      "--- 3. 최적 포트폴리오 계산 (L2_reg gamma 헷징) ---\n",
      "\n",
      "최적의 포트폴리오 비율 (L2_reg 헷징 적용):\n",
      "            Weight Weight (%)\n",
      "BROADCOM   0.00000      0.00%\n",
      "ALPHABET   0.00000      0.00%\n",
      "AMAZON     0.00000      0.00%\n",
      "APPLE      0.00000      0.00%\n",
      "META       0.86699     86.70%\n",
      "MICROSOFT  0.00000      0.00%\n",
      "NETFLIX    0.13301     13.30%\n",
      "NVIDIA     0.00000      0.00%\n",
      "PALANTIR   0.00000      0.00%\n",
      "TESLA      0.00000      0.00%\n",
      "\n",
      "예상 포트폴리오 성과 (통합 모델 예측 mu 기준):\n",
      "Expected annual return: 440.3%\n",
      "Annual volatility: 40.8%\n",
      "Sharpe Ratio: 10.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/quant/lib/python3.8/site-packages/pypfopt/efficient_frontier/efficient_frontier.py:259: UserWarning: max_sharpe transforms the optimization problem so additional objectives may not work as expected.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "csv_files = [\n",
    "    'BROADCOM 5년치.csv', 'ALPHABET C 5년치.csv', 'AMAZON 5년치.csv', \n",
    "    'APPLE 5년치.csv', 'META 5년치.csv', 'MICROSOFT 5년치.csv', \n",
    "    'NETFLIX 5년치.csv', 'NVIDIA 5년치.csv', 'PALANTIR 5년치.csv', 'TESLA 5년치.csv'\n",
    "]\n",
    "stock_names = [re.match(r'^[A-Z]+', f).group(0) for f in csv_files]\n",
    "feature_cols = ['Close/Last', 'Volume', 'Open', 'High', 'Low']\n",
    "num_features = len(csv_files) * len(feature_cols) # 50\n",
    "\n",
    "input_len = 1024\n",
    "output_len = 14\n",
    "# [!!!] hidden_dim을 128로 수정 (오류 해결)\n",
    "hidden_dim = 128\n",
    "\n",
    "model_path = \"unified_model.pt\"\n",
    "scaler_path = \"unified_scaler.pkl\"\n",
    "column_map_path = \"unified_column_map.pkl\"\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# --- 2. 모델, 스케일러, 컬럼 맵 로드 ---\n",
    "print(\"--- 모델(hidden_dim=128) 및 스케일러 로드 ---\")\n",
    "model = iTransformer(\n",
    "    input_len=input_len, \n",
    "    output_len=output_len, \n",
    "    num_features=num_features, # 50\n",
    "    hidden_dim=hidden_dim, # 128\n",
    "    num_heads=8, \n",
    "    num_layers=6\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "with open(scaler_path, 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "with open(column_map_path, 'rb') as f:\n",
    "    column_names = pickle.load(f) # 50개 컬럼 이름 리스트\n",
    "\n",
    "# --- 3. 추론용 데이터 준비 (훈련 시와 동일하게) ---\n",
    "# ... (이전과 동일한 데이터 준비 코드) ...\n",
    "print(\"--- 추론용 데이터 준비 ---\")\n",
    "all_data_dfs = []\n",
    "for csv_file in csv_files:\n",
    "    stock_name = re.match(r'^[A-Z]+', csv_file).group(0)\n",
    "    df = pd.read_csv(csv_file, parse_dates=['Date'], index_col='Date')\n",
    "    df = df.sort_index(ascending=True)\n",
    "    \n",
    "    cleaned_cols = {}\n",
    "    for col in feature_cols:\n",
    "        series = df[col]\n",
    "        if series.dtype == 'object':\n",
    "            series = series.str.replace(r'[$,]', '', regex=True)\n",
    "        series = pd.to_numeric(series, errors='coerce')\n",
    "        cleaned_cols[f\"{stock_name}_{col}\"] = series\n",
    "    all_data_dfs.append(pd.DataFrame(cleaned_cols))\n",
    "\n",
    "all_data_wide_df = pd.concat(all_data_dfs, axis=1)\n",
    "all_data_wide_df = all_data_wide_df[column_names]\n",
    "all_data_wide_df = all_data_wide_df.dropna()\n",
    "all_data_np = all_data_wide_df.to_numpy(dtype=np.float32)\n",
    "scaled_all_data_np = scaler.transform(all_data_np)\n",
    "backtest_input = scaled_all_data_np[-input_len:] # (1024, 50)\n",
    "new_input_tensor = torch.tensor(backtest_input, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "# --- 4. 예측 및 역정규화 ---\n",
    "print(\"--- 예측 수행 ---\")\n",
    "with torch.no_grad():\n",
    "    scaled_output = model(new_input_tensor) # (1, 14, 50)\n",
    "\n",
    "original_scale_output = scaler.inverse_transform(scaled_output.squeeze(0).cpu().numpy())\n",
    "original_input_data = scaler.inverse_transform(backtest_input)\n",
    "\n",
    "# --- 5. 기대수익률 (mu) 추출 ---\n",
    "print(\"--- 1. 기대수익률 (mu) 계산 ---\")\n",
    "predicted_mus = {}\n",
    "\n",
    "for stock_name in stock_names:\n",
    "    close_col_name = f\"{stock_name}_Close/Last\"\n",
    "    close_index = column_names.index(close_col_name)\n",
    "    current_price = original_input_data[-1, close_index]\n",
    "    predicted_future_price = original_scale_output[-1, close_index]\n",
    "\n",
    "    ratio_14day = (predicted_future_price / current_price) - 1\n",
    "    annualized_mu = ((1 + ratio_14day) ** (252.0 / output_len)) - 1\n",
    "    \n",
    "    print(f\"  [{stock_name}] 현재가: {current_price:.2f}, 14일 후 예측: {predicted_future_price:.2f} -> 연 {annualized_mu*100:.2f}%\")\n",
    "    predicted_mus[stock_name] = annualized_mu\n",
    "\n",
    "# --- 6. 위험 (S) 계산 ---\n",
    "print(\"--- 2. 위험 공분산 (S) 계산 (과거 종가 데이터 사용) ---\")\n",
    "close_price_df = pd.DataFrame()\n",
    "for stock_name in stock_names:\n",
    "    close_col_name = f\"{stock_name}_Close/Last\"\n",
    "    close_price_df[stock_name] = all_data_wide_df[close_col_name]\n",
    "\n",
    "S = risk_models.sample_cov(close_price_df, frequency=252)\n",
    "mu_series = pd.Series(predicted_mus)\n",
    "\n",
    "# --- 7. 포트폴리오 최적화 (L2_reg 헷징) ---\n",
    "print(\"--- 3. 최적 포트폴리오 계산 (L2_reg gamma 헷징) ---\")\n",
    "ef = EfficientFrontier(mu_series, S)\n",
    "\n",
    "# [!!!] 헷징 방식 수정: L2 정규화를 '목표'로 추가 (gamma=1.0)\n",
    "# gamma 값을 높일수록 분산 효과(헷징)가 강해집니다.\n",
    "ef.add_objective(objective_functions.L2_reg, gamma=10.0)\n",
    "\n",
    "try:\n",
    "    weights = ef.max_sharpe(risk_free_rate=0.02)\n",
    "    cleaned_weights = ef.clean_weights()\n",
    "\n",
    "    print(\"\\n최적의 포트폴리오 비율 (L2_reg 헷징 적용):\")\n",
    "    weights_df = pd.DataFrame.from_dict(cleaned_weights, orient='index', columns=['Weight'])\n",
    "    weights_df['Weight (%)'] = weights_df['Weight'].apply(lambda x: f\"{x*100:.2f}%\")\n",
    "    print(weights_df)\n",
    "\n",
    "    print(\"\\n예상 포트폴리오 성과 (통합 모델 예측 mu 기준):\")\n",
    "    ef.portfolio_performance(verbose=True, risk_free_rate=0.02)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"최적화 오류: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (quant)",
   "language": "python",
   "name": "quant"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
